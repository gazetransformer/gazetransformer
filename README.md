### Hi there ðŸ‘‹
### This is the support code for the "Temporal Understanding of Gaze Communication with GazeTransformer".

## Dataset (VACATION+) ##

The VACATION+ folder contains the annotations of the modified VACATION dataset.

Please [contact the authors of the original VACATION dataset](https://github.com/LifengFan/Human-Gaze-Communication) to access the corresponding video.

You will need to fill out a Google form to request access.

## Training (in progress) ##

## Evaluation (in progress) ##

## Demo ##
We aim to simultaneously predict head and object locations and infer their corresponding gaze relationships in an end-to-end manner.

Unlike previous end-to-end models that can only infer attended targets or handle Mutual gaze relationships, our GazeTransformer can predict Single, Share and Mutual atomic-level behaviours (see below).

![Alt Text](sample_output/64.gif)

Sample results of our model can be found in the 'sample_output' folder.

The dotted rectangles represent ground truth human and object locations, while solid rectangles represent model predictions.

<!--
**gazetransformer/gazetransformer** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
